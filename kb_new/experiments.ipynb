{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f2f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# BASIC DEFINITIONS\n",
    "level1_representative_subset_problem_ids = [1, 3, 6, 18, 23, 26, 33, 36, 40, 42, 48, 54, 57, 65, 77, 82, 86, 87]\n",
    "level2_representative_subset_problem_ids = [1, 2, 8, 18, 23, 28, 33, 43]\n",
    "level3_representative_subset_problem_ids = [1, 5, 8, 11, 20, 33, 38, 43]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbed7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelExecResult(BaseModel):\n",
    "    \"\"\"\n",
    "    Single Kernel Execution\n",
    "    \"\"\"\n",
    "\n",
    "    compiled: bool = False\n",
    "    correctness: bool = False\n",
    "    metadata: dict = {}\n",
    "    runtime: float = -1.0  # in us, only recorded if we decide to measure performance\n",
    "    runtime_stats: dict = {}  # only recorded if we decide to measure performance\n",
    "\n",
    "\n",
    "# This is from the \"eval.py\" file.\n",
    "def set_seed(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    # NOTE: this only sets on current cuda device\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    \n",
    "def load_original_model_and_inputs(\n",
    "    model_original_src: str, context: dict\n",
    ") -> tuple[nn.Module, callable, callable]:\n",
    "    \"\"\"\n",
    "    Load class from original NN.module pytorch code\n",
    "    this is pytorch reference and we feed that to model to see if there will be any improvement\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        compile(model_original_src, \"<string>\", \"exec\")\n",
    "    except SyntaxError as e:\n",
    "        print(f\"Syntax Error in original code {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        exec(model_original_src, context)  # expose to current namespace\n",
    "    except Exception as e:\n",
    "        print(f\"Error in executing original code {e}\")\n",
    "        return None\n",
    "\n",
    "    # these should be defined in the original model code and present in the context\n",
    "    get_init_inputs_fn = context.get(\"get_init_inputs\")\n",
    "    get_inputs_fn = context.get(\"get_inputs\")\n",
    "    Model = context.get(\"Model\")\n",
    "    return (Model, get_init_inputs_fn, get_inputs_fn)\n",
    "\n",
    "def load_custom_model(\n",
    "    model_custom_src: str, context: dict, build_directory: str = None\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Load class from custom NN.module pytorch code\n",
    "    this is the code output by LLM with calls to custom cuda kernels\n",
    "    \"\"\"\n",
    "    if build_directory:\n",
    "        context[\"BUILD_DIRECTORY\"] = build_directory\n",
    "        # Add import at the start of the source code\n",
    "        model_custom_src = (\n",
    "            \"import os\\n\" f\"os.environ['TORCH_EXTENSIONS_DIR'] = '{build_directory}'\\n\"\n",
    "        ) + model_custom_src\n",
    "\n",
    "    try:\n",
    "        compile(model_custom_src, \"<string>\", \"exec\")\n",
    "        exec(model_custom_src, context)\n",
    "        # DANGER: need to delete refernece from global namespace\n",
    "    except SyntaxError as e:\n",
    "        print(f\"Syntax Error in custom generated code or Compilation Error {e}\")\n",
    "        return None\n",
    "\n",
    "    ModelNew = context.get(\"ModelNew\")\n",
    "    return ModelNew\n",
    "\n",
    "\n",
    "def time_execution_with_cuda_event(\n",
    "    kernel_fn: callable,\n",
    "    *args,\n",
    "    num_warmup: int = 3,\n",
    "    num_trials: int = 10,\n",
    "    verbose: bool = True,\n",
    "    device: torch.device = None,\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Time a CUDA kernel function over multiple trials using torch.cuda.Event\n",
    "\n",
    "    Args:\n",
    "        kernel_fn: Function to time\n",
    "        *args: Arguments to pass to kernel_fn\n",
    "        num_trials: Number of timing trials to run\n",
    "        verbose: Whether to print per-trial timing info\n",
    "        device: CUDA device to use, if None, use current device\n",
    "\n",
    "    Returns:\n",
    "        List of elapsed times in milliseconds\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        if verbose:\n",
    "            print(f\"Using current device: {torch.cuda.current_device()}\")\n",
    "        device = torch.cuda.current_device()\n",
    "\n",
    "    # Warm ups\n",
    "    for _ in range(num_warmup):\n",
    "        kernel_fn(*args)\n",
    "        torch.cuda.synchronize(device=device)\n",
    "\n",
    "    print(\n",
    "        f\"[Profiling] Using device: {device} {torch.cuda.get_device_name(device)}, warm up {num_warmup}, trials {num_trials}\"\n",
    "    )\n",
    "    elapsed_times = []\n",
    "\n",
    "    # Actual trials\n",
    "    for trial in range(num_trials):\n",
    "        # create event marker default is not interprocess\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start_event.record()\n",
    "        kernel_fn(*args)\n",
    "        end_event.record()\n",
    "\n",
    "        # Synchronize to ensure the events have completed\n",
    "        torch.cuda.synchronize(device=device)\n",
    "\n",
    "        # Calculate the elapsed time in milliseconds\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "        if verbose:\n",
    "            print(f\"Trial {trial + 1}: {elapsed_time_ms:.3g} ms\")\n",
    "        elapsed_times.append(elapsed_time_ms)\n",
    "\n",
    "    return elapsed_times\n",
    "\n",
    "def get_timing_stats(elapsed_times: list[float], device: torch.device = None) -> dict:\n",
    "    \"\"\"Get timing statistics from a list of elapsed times.\n",
    "\n",
    "    Args:\n",
    "        elapsed_times: List of elapsed times in milliseconds\n",
    "        device: CUDA device, record device info\n",
    "    Returns:\n",
    "        Dict containing mean, std, min, max and num_trials\n",
    "        all timing are in ms\n",
    "    \"\"\"\n",
    "\n",
    "    stats = {\n",
    "        \"mean\": float(f\"{np.mean(elapsed_times):.3g}\"),\n",
    "        \"std\": float(f\"{np.std(elapsed_times):.3g}\"),\n",
    "        \"min\": float(f\"{np.min(elapsed_times):.3g}\"),\n",
    "        \"max\": float(f\"{np.max(elapsed_times):.3g}\"),\n",
    "        \"num_trials\": len(elapsed_times),\n",
    "    }\n",
    "\n",
    "    if device:\n",
    "        stats[\"hardware\"] = torch.cuda.get_device_name(device=device)\n",
    "        stats[\"device\"] = str(device)  # for debugging\n",
    "\n",
    "    return stats\n",
    "\n",
    "# From the original file \"generate_baseline_time.py\"\n",
    "def measure_program_time(\n",
    "        ref_arch_name: str,\n",
    "        ref_arch_src: str, \n",
    "        num_trials: int = 100,\n",
    "        use_torch_compile: bool = False,\n",
    "        torch_compile_backend: str=\"inductor\", \n",
    "        torch_compile_options: str=\"default\",\n",
    "        device: torch.device=\"cuda:0\",\n",
    "        verbose: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Measure the time of a KernelBench reference architecture\n",
    "    \"\"\"\n",
    "    context = {}\n",
    "    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(\n",
    "        ref_arch_src, context\n",
    "    )\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.synchronize(device=device)\n",
    "            set_seed(42)\n",
    "            inputs = get_inputs()\n",
    "            set_seed(42)\n",
    "            init_inputs = get_init_inputs()\n",
    "            inputs = [\n",
    "                x.cuda(device=device) if isinstance(x, torch.Tensor) else x\n",
    "                for x in inputs\n",
    "            ]\n",
    "            init_inputs = [\n",
    "                x.cuda(device=device) if isinstance(x, torch.Tensor) else x\n",
    "                for x in init_inputs\n",
    "            ]\n",
    "\n",
    "            # Initialize PyTorch model, use this for eager mode execution\n",
    "            model = Model(*init_inputs)\n",
    "            \n",
    "            if use_torch_compile:\n",
    "                print(f\"Using torch.compile to compile model {ref_arch_name} with {torch_compile_backend} backend and {torch_compile_options} mode\")\n",
    "                model = torch.compile(model, backend=torch_compile_backend, mode=torch_compile_options)\n",
    "            else:\n",
    "                print(f\"Using PyTorch Eager Execution on {ref_arch_name}\")\n",
    "            \n",
    "            model = model.cuda(device=device)\n",
    "            torch.cuda.synchronize(device=device)\n",
    "            elapsed_times = time_execution_with_cuda_event(\n",
    "                model, *inputs, num_trials=num_trials, verbose=verbose, device=device\n",
    "            )\n",
    "            runtime_stats = get_timing_stats(elapsed_times, device=device)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{ref_arch_name} {runtime_stats}\")\n",
    "            \n",
    "            return runtime_stats\n",
    "    except Exception as e:\n",
    "        print(f\"[Eval] Error in Measuring Performance: {e}\")\n",
    "\n",
    "\n",
    "#Additional Functions (eval.py and others?)\n",
    "def gpu_cache_clean(device: torch.device = torch.device(\"cuda:0\")):\n",
    "    \"\"\"Cleans up the GPU cache.\"\"\"\n",
    "    # Clear CUDA cache and reset GPU state\n",
    "    with torch.cuda.device(device):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # does this help?\n",
    "        torch.cuda.reset_peak_memory_stats(device=device)\n",
    "\n",
    "        torch.cuda.synchronize(\n",
    "            device=device\n",
    "        )  # Wait for all CUDA operations to complete\n",
    "\n",
    "def graceful_eval_cleanup(curr_context: dict, device: torch.device):\n",
    "    \"\"\"\n",
    "    Clean up env, gpu cache, and compiled CUDA extensions after evaluation\n",
    "    \"\"\"  # delete ran-specific function definitions before next eval run\n",
    "    del curr_context\n",
    "    gpu_cache_clean(device=device)\n",
    "\n",
    "\n",
    "def delete_tensor(tensor):\n",
    "    \"\"\"\n",
    "    Deletes the tensors...\n",
    "    \"\"\"\n",
    "    del tensor\n",
    "\n",
    "\n",
    "def run_and_check_correctness(\n",
    "    original_model_instance: nn.Module,\n",
    "    new_model_instance: nn.Module,\n",
    "    get_inputs_fn: callable,\n",
    "    metadata: dict,\n",
    "    num_correct_trials: int,\n",
    "    verbose=False,\n",
    "    seed=42,\n",
    "    device=None,\n",
    ") -> KernelExecResult:\n",
    "    \"\"\"\n",
    "    run the model and check correctness,\n",
    "    assume model already loaded and compiled (loaded and compiled in the caller)\n",
    "    this is all on GPU, requiring cuda device and transfer .cuda()\n",
    "\n",
    "    num_correct_trials: run the evalutation multiple times with (ideally) different random inputs to ensure correctness\n",
    "    \"\"\"\n",
    "    pass_count = 0\n",
    "\n",
    "    # Generate num_correct_trials seeds deterministically from the initial seed\n",
    "    torch.manual_seed(seed)\n",
    "    correctness_trial_seeds = [\n",
    "        torch.randint(0, 2**32 - 1, (1,)).item() for _ in range(num_correct_trials)\n",
    "    ]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for trial in range(num_correct_trials):\n",
    "\n",
    "            trial_seed = correctness_trial_seeds[trial]\n",
    "            if verbose:\n",
    "                print(f\"[Eval] Generating Random Input with seed {trial_seed}\")\n",
    "\n",
    "            set_seed(trial_seed)\n",
    "            inputs = get_inputs_fn()\n",
    "            inputs = [\n",
    "                x.cuda(device=device) if isinstance(x, torch.Tensor) else x\n",
    "                for x in inputs\n",
    "            ]\n",
    "\n",
    "            set_seed(trial_seed)\n",
    "            model = original_model_instance.cuda(device=device)\n",
    "\n",
    "            set_seed(trial_seed)\n",
    "            model_new = new_model_instance.cuda(device=device)\n",
    "\n",
    "            output = model(*inputs)\n",
    "            torch.cuda.synchronize(device=device)\n",
    "            # ensure all GPU operations are completed before checking results\n",
    "\n",
    "            try:\n",
    "                output_new = model_new(*inputs)\n",
    "                torch.cuda.synchronize(device=device)\n",
    "                if output.shape != output_new.shape:\n",
    "                    metadata = register_and_format_exception(\n",
    "                        \"correctness_issue\",\n",
    "                        f\"Output shape mismatch: Expected {output.shape}, got {output_new.shape}\",\n",
    "                        metadata,\n",
    "                    )\n",
    "                    if verbose:\n",
    "                        print(\n",
    "                            f\"[FAIL] trial {trial}: Output shape mismatch: Expected {output.shape}, got {output_new.shape}\"\n",
    "                        )\n",
    "                    return KernelExecResult(\n",
    "                        compiled=True, correctness=False, metadata=metadata\n",
    "                    )\n",
    "\n",
    "                # check output value difference\n",
    "                if not torch.allclose(\n",
    "                    output, output_new, atol=1e-02, rtol=1e-02\n",
    "                ):  # fail\n",
    "                    max_diff = torch.max(torch.abs(output - output_new)).item()\n",
    "                    avg_diff = torch.mean(torch.abs(output - output_new)).item()\n",
    "                    metadata.setdefault(\"max_difference\", []).append(f\"{max_diff:.6f}\")\n",
    "                    metadata.setdefault(\"avg_difference\", []).append(f\"{avg_diff:.6f}\")\n",
    "                    metadata[\"correctness_issue\"] = \"Output mismatch\"\n",
    "                    if verbose:\n",
    "                        print(f\"[FAIL] trial {trial}: Output mismatch\")\n",
    "                else:  # pass\n",
    "                    pass_count += 1\n",
    "                    if verbose:\n",
    "                        print(f\"[PASS] trial {trial}: New Model matches Model\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"[Error] Exception happens during correctness check\")\n",
    "                print(f\"Error in launching kernel for ModelNew: {e}\")\n",
    "\n",
    "                metadata = register_and_format_exception(\n",
    "                    \"runtime_error\", e, metadata, truncate=True\n",
    "                )\n",
    "                return KernelExecResult(\n",
    "                    compiled=True, correctness=False, metadata=metadata\n",
    "                )\n",
    "                # break\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"[Eval] Pass count: {pass_count}, num_correct_trials: {num_correct_trials}\"\n",
    "        )\n",
    "\n",
    "    # put all the useful info here!\n",
    "    metadata[\"correctness_trials\"] = f\"({pass_count} / {num_correct_trials})\"\n",
    "\n",
    "    if pass_count == num_correct_trials:\n",
    "        return KernelExecResult(compiled=True, correctness=True, metadata=metadata)\n",
    "    else:\n",
    "        return KernelExecResult(compiled=True, correctness=False, metadata=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ae66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_kernel_against_ref(\n",
    "    original_model_src: str,\n",
    "    custom_model_src: str,\n",
    "    seed_num: int = 42,\n",
    "    num_correct_trials: int = 1,\n",
    "    num_perf_trials: int = 10,\n",
    "    verbose: bool = False,\n",
    "    measure_performance: bool = False,\n",
    "    build_dir: os.PathLike = None,\n",
    "    device: torch.device = torch.cuda.current_device() if torch.cuda.is_available() else None, # have to run on GPU\n",
    ") -> KernelExecResult:\n",
    "    \"\"\"\n",
    "    Evaluate the custom kernel against the original model\n",
    "\n",
    "    num_correct_trials: number of trials to initialize different random inputs; correctness pass only if all trials pass\n",
    "    num_perf_trials: run the evalutation many times to take the average\n",
    "    device: GPU (cuda) device to run the evalutation on\n",
    "    \"\"\"\n",
    "    # TODO: check device is busy\n",
    "    assert torch.cuda.is_available(), \"CUDA is not available, cannot run Eval\"\n",
    "    torch.set_printoptions(\n",
    "        precision=4,  # Decimal places\n",
    "        threshold=10,  # Total number of elements before truncating\n",
    "        edgeitems=3,  # Number of elements at beginning and end of dimensions\n",
    "        linewidth=80,  # Maximum width before wrapping\n",
    "    )\n",
    "\n",
    "    # set CUDA device\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "    context = {}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[Eval] Start Evalulation! on device: {device}\")\n",
    "        print(\"[Eval] Loading Original Model\")\n",
    "\n",
    "    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(\n",
    "        original_model_src, context\n",
    "    )\n",
    "    set_seed(seed_num)  # set seed for reproducible input\n",
    "    init_inputs = get_init_inputs()\n",
    "    init_inputs = [\n",
    "        x.cuda(device=device) if isinstance(x, torch.Tensor) else x for x in init_inputs\n",
    "    ]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        set_seed(seed_num)  # set seed for reproducible weights\n",
    "        original_model = Model(*init_inputs)\n",
    "        assert hasattr(original_model, \"forward\")\n",
    "        if verbose:\n",
    "            print(\"[Eval] Original Model Loaded\")\n",
    "    if verbose:\n",
    "        print(\"[Eval] Loading and Compiling New Model with Custom CUDA Kernel\")\n",
    "\n",
    "    metadata = {}  # for storing result metadata\n",
    "    metadata[\"hardware\"] = torch.cuda.get_device_name(device=device)\n",
    "    metadata[\"device\"] = str(device)  # for debugging\n",
    "\n",
    "    # this is where compilation happens\n",
    "    try:\n",
    "        os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"  # compile with device side assertion\n",
    "        # add hash for later to distinguish between multi-turn kernels\n",
    "        ModelNew = load_custom_model(custom_model_src, context, build_dir)\n",
    "        torch.cuda.synchronize(device=device)  # not sure if this is too much\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Failed to compile custom CUDA kernel: Record as compilation failure. \\nError: {e}\"\n",
    "        )\n",
    "        # TODO: add metadata for compilation error (how to we get the compilation error message?)\n",
    "\n",
    "        if \"lock\" in str(e) or \"No such file or directory\" in str(e):\n",
    "            # this is a lock file error, likely due to concurrent compilation\n",
    "            # this does not necessarily mean the compilation failed, but we should retry\n",
    "            print(\n",
    "                f\"[Eval] Lock file error during compilation, Please retry. Error: {e}\"\n",
    "            )\n",
    "            graceful_eval_cleanup(context, device)\n",
    "            return None\n",
    "        else:\n",
    "            metadata[\"compilation_error\"] = e\n",
    "            graceful_eval_cleanup(context, device)\n",
    "            return KernelExecResult(\n",
    "                compiled=False, metadata=metadata\n",
    "            )  # skip further steps\n",
    "\n",
    "    # at this point we passed compilation\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            set_seed(seed_num)  # set seed for reproducible weights\n",
    "            custom_model = ModelNew(*init_inputs)\n",
    "            assert hasattr(custom_model, \"forward\")\n",
    "            torch.cuda.synchronize(device=device)\n",
    "        if verbose:\n",
    "            print(\"[Eval] New Model with Custom CUDA Kernel Loaded\")\n",
    "    except RuntimeError as e:\n",
    "        print(\n",
    "            f\"Failed to load custom CUDA kernel; Compiled but not able to run, count as runtime error. \\nError: {e}\"\n",
    "        )\n",
    "        # TODO: add metadata for runtime error e.g. error in launching kernel, illegal memory access, ...\n",
    "        graceful_eval_cleanup(context, device)\n",
    "        metadata[\"runtime_error\"] = e\n",
    "        return KernelExecResult(\n",
    "            compiled=True, correctness=False, metadata=metadata\n",
    "        )  # skip further steps\n",
    "\n",
    "    kernel_exec_result = None\n",
    "\n",
    "    # Check Correctness\n",
    "    if verbose:\n",
    "        print(\"[Eval] Checking Correctness\")\n",
    "    try:\n",
    "        kernel_exec_result = run_and_check_correctness(\n",
    "            original_model,\n",
    "            custom_model,\n",
    "            get_inputs,\n",
    "            metadata=metadata,\n",
    "            num_correct_trials=num_correct_trials,\n",
    "            verbose=verbose,\n",
    "            seed=seed_num,\n",
    "            device=device,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # TODO: add metadata for runtime error e.g. error in launching kernel, illegal memory access, ...\n",
    "        metadata[\"runtime_error\"] = e\n",
    "        kernel_exec_result = KernelExecResult(\n",
    "            compiled=True, correctness=False, metadata=metadata\n",
    "        )\n",
    "\n",
    "    # Measure Performance [Optional] | conditioned on compilation + correctness + no exception so far\n",
    "    if measure_performance:\n",
    "        try:\n",
    "            if kernel_exec_result and kernel_exec_result.correctness:\n",
    "                if verbose:\n",
    "                    print(\"[Eval] Measuring Performance as Sample is Correct\")\n",
    "\n",
    "                torch.cuda.synchronize(device=device)\n",
    "                set_seed(seed_num)\n",
    "                inputs = get_inputs()\n",
    "                inputs = [\n",
    "                    x.cuda(device=device) if isinstance(x, torch.Tensor) else x\n",
    "                    for x in inputs\n",
    "                ]\n",
    "                model_new = custom_model.cuda(device=device)\n",
    "                torch.cuda.synchronize(device=device)\n",
    "\n",
    "                elapsed_times = time_execution_with_cuda_event(\n",
    "                    model_new,\n",
    "                    *inputs,\n",
    "                    num_trials=num_perf_trials,\n",
    "                    verbose=verbose,\n",
    "                    device=device,\n",
    "                )\n",
    "                runtime_stats = get_timing_stats(elapsed_times, device=device)\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[Eval] Performance Stats: {runtime_stats}\")\n",
    "                kernel_exec_result.runtime = runtime_stats[\"mean\"]\n",
    "                kernel_exec_result.runtime_stats = runtime_stats\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"[Eval] Error in Measuring Performance: {e}\")\n",
    "            kernel_exec_result.metadata[\"error_during_performance\"] = e\n",
    "\n",
    "    graceful_eval_cleanup(context, device)\n",
    "    return kernel_exec_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f5fa2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0915 14:57:13.956000 298 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "W0915 14:57:13.956000 298 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Start Evalulation! on device: 0\n",
      "[Eval] Loading Original Model\n",
      "[Eval] Original Model Loaded\n",
      "[Eval] Loading and Compiling New Model with Custom CUDA Kernel\n",
      "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=custom_mm_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -isystem /workspace/z_code/kernelbench/kb_new/env_kb/lib/python3.11/site-packages/torch/include -isystem /workspace/z_code/kernelbench/kb_new/env_kb/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -fPIC -std=c++17 -c /root/.cache/torch_extensions/py311_cu128/custom_mm/main.cpp -o main.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=custom_mm_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -isystem /workspace/z_code/kernelbench/kb_new/env_kb/lib/python3.11/site-packages/torch/include -isystem /workspace/z_code/kernelbench/kb_new/env_kb/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -arch=sm_70 --use_fast_math -std=c++17 -c /root/.cache/torch_extensions/py311_cu128/custom_mm/cuda.cu -o cuda.cuda.o \n",
      "nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "[3/3] c++ main.o cuda.cuda.o -shared -L/workspace/z_code/kernelbench/kb_new/env_kb/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o custom_mm_v1.so\n",
      "[Eval] New Model with Custom CUDA Kernel Loaded\n",
      "[Eval] Checking Correctness\n",
      "[Eval] Generating Random Input with seed 734796314\n",
      "[PASS] trial 0: New Model matches Model\n",
      "[Eval] Pass count: 1, num_correct_trials: 1\n",
      "[Eval] Measuring Performance as Sample is Correct\n",
      "[Profiling] Using device: 0 NVIDIA A40, warm up 3, trials 3\n",
      "Trial 1: 47.6 ms\n",
      "Trial 2: 47.5 ms\n",
      "Trial 3: 47.6 ms\n",
      "[Eval] Performance Stats: {'mean': 47.6, 'std': 0.07, 'min': 47.5, 'max': 47.6, 'num_trials': 3}\n",
      "compiled=True correctness=True metadata={'hardware': 'NVIDIA A40', 'device': '0', 'correctness_trials': '(1 / 1)'} runtime=47.6 runtime_stats={'mean': 47.6, 'std': 0.07, 'min': 47.5, 'max': 47.6, 'num_trials': 3}\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT TO Evaluate performance...\n",
    "def read_file(file_path) -> str:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_path} does not exist\")\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "gpu_cache_clean()\n",
    "# TENSORS_DIR = \"./tensors\"\n",
    "# file_name: str= \"{idx}.json\"\n",
    "\n",
    "\n",
    "# MAIN SCRIPT\n",
    "# features: ['code', 'level', 'name', 'problem_id'],\n",
    "ds = load_dataset(\"ai-nikolai/KernelBench\")\n",
    "level=1\n",
    "actual_ds = ds[f\"level_{level}\"]\n",
    "actual_ds = actual_ds.filter(lambda x: x[\"problem_id\"] in [1]) #level1_representative_subset_problem_ids)\n",
    "num_problems = len(actual_ds)\n",
    "\n",
    "custom_src = read_file(\"./kb_solutions/level1/1_Square_matrix_multiplication_.py\")\n",
    "\n",
    "sample = actual_ds[0]\n",
    "\n",
    "results = eval_kernel_against_ref(\n",
    "    original_model_src = sample[\"code\"],\n",
    "    custom_model_src = custom_src,\n",
    "    seed_num = 42,\n",
    "    num_correct_trials = 1,\n",
    "    num_perf_trials = 3,\n",
    "    verbose  = True,\n",
    "    measure_performance = True,\n",
    "    build_dir = None,\n",
    "    device = torch.cuda.current_device() if torch.cuda.is_available() else None, # have to run on GPU\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3952f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_input_output(ref_arch_src, device: torch.device=\"cuda:0\",seed:int=123):\n",
    "    \"\"\"Creates Init, Input and Output for a given model.\"\"\"\n",
    "    context = {}\n",
    "    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(\n",
    "        ref_arch_src, context\n",
    "    )\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # gpu_cache_clean()\n",
    "            torch.cuda.synchronize(device=device)\n",
    "            \n",
    "            set_seed(seed)\n",
    "            init = get_init_inputs()\n",
    "            init = [\n",
    "                x.cuda(device=device) if isinstance(x, torch.Tensor) else x\n",
    "                for x in init\n",
    "            ]\n",
    "\n",
    "            set_seed(seed)\n",
    "            inputs = get_inputs()\n",
    "            inputs = [\n",
    "                x.cuda(device=device) if isinstance(x, torch.Tensor) else x\n",
    "                for x in inputs\n",
    "            ]\n",
    "\n",
    "            # Initialize PyTorch model, use this for eager mode execution\n",
    "            model = Model(*init)           \n",
    "            model = model.cuda(device=device)\n",
    "            \n",
    "            set_seed(seed)\n",
    "            torch.cuda.synchronize(device=device)\n",
    "            outputs = model(*inputs)\n",
    "            torch.cuda.synchronize(device=device)\n",
    "\n",
    "\n",
    "            # init_numpy = [\n",
    "            #     x.numpy(force=True) if isinstance(x, torch.Tensor) else x\n",
    "            #     for x in init\n",
    "            # ]\n",
    "            # inputs_numpy = [\n",
    "            #     x.numpy(force=True) if isinstance(x, torch.Tensor) else x\n",
    "            #     for x in inputs\n",
    "            # ]\n",
    "            # outputs_numpy = outputs.numpy(force=True)\n",
    "\n",
    "            init_numpy = [\n",
    "                x.tolist() if isinstance(x, torch.Tensor) else x\n",
    "                for x in init\n",
    "            ]\n",
    "            inputs_numpy = [\n",
    "                x.tolist() if isinstance(x, torch.Tensor) else x\n",
    "                for x in inputs\n",
    "            ]\n",
    "            outputs_numpy = outputs.tolist()\n",
    "\n",
    "            delete_tensor(init)\n",
    "            delete_tensor(inputs)\n",
    "            delete_tensor(outputs)\n",
    "\n",
    "            graceful_eval_cleanup(context, device)\n",
    "\n",
    "            return init_numpy, inputs_numpy, outputs_numpy\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Eval] Error in Getting Init, Input, Output: {e}\")\n",
    "\n",
    "# if not torch.allclose(output, output_new, atol=1e-02, rtol=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544b6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 2/18 [01:52<14:36, 54.80s/it]"
     ]
    }
   ],
   "source": [
    "# SCRIPT TO SAME INPUTS & OUTPUTS.\n",
    "gpu_cache_clean()\n",
    "TENSORS_DIR = \"./tensors\"\n",
    "file_name: str= \"{idx}.json\"\n",
    "\n",
    "\n",
    "# MAIN SCRIPT\n",
    "# features: ['code', 'level', 'name', 'problem_id'],\n",
    "ds = load_dataset(\"ai-nikolai/KernelBench\")\n",
    "level=1\n",
    "actual_ds = ds[f\"level_{level}\"]\n",
    "actual_ds = actual_ds.filter(lambda x: x[\"problem_id\"] in level1_representative_subset_problem_ids)\n",
    "num_problems = len(actual_ds)\n",
    "\n",
    "\n",
    "#SOME INIT PARAMS\n",
    "num_trials: int= 3\n",
    "\n",
    "use_torch_compile: bool = False\n",
    "torch_compile_backend: str=\"inductor\"\n",
    "torch_compile_options: str=\"default\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "\n",
    "def map_init_inputs_outputs(sample):\n",
    "    ref_arch_src, ref_arch_name  = sample[\"code\"], sample[\"name\"]\n",
    "    try:\n",
    "        init, inputs, outputs = get_init_input_output(ref_arch_src, device=device, seed=1)\n",
    "        return {\n",
    "            \"init\": init,\n",
    "            \"inputs\": inputs,\n",
    "            \"outputs\": outputs\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"Failed Execution\")\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "for sample in tqdm.tqdm(actual_ds):\n",
    "    data = map_init_inputs_outputs(sample)\n",
    "\n",
    "    pid = sample[\"problem_id\"]\n",
    "    save_path = os.path.join(TENSORS_DIR, f'level_{level}', file_name.format(idx=pid))\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT FOR TIMING\n",
    "gpu_cache_clean()\n",
    "TIMING_DIR = \"./timings\"\n",
    "file_name: str=\"baseline_time_v2.json\"\n",
    "\n",
    "\n",
    "# MAIN SCRIPT\n",
    "# features: ['code', 'level', 'name', 'problem_id'],\n",
    "ds = load_dataset(\"ai-nikolai/KernelBench\")\n",
    "level=1\n",
    "actual_ds = ds[f\"level_{level}\"]\n",
    "actual_ds = actual_ds.filter(lambda x: x[\"problem_id\"] in level1_representative_subset_problem_ids)\n",
    "num_problems = len(actual_ds)\n",
    "\n",
    "\n",
    "#SOME INIT PARAMS\n",
    "num_trials: int= 3\n",
    "\n",
    "use_torch_compile: bool = False\n",
    "torch_compile_backend: str=\"inductor\"\n",
    "torch_compile_options: str=\"default\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "json_results = {}\n",
    "\n",
    "json_results[f\"level_{level}\"]={}\n",
    "\n",
    "for sample in tqdm.tqdm(actual_ds):\n",
    "    ref_arch_src, ref_arch_name  = sample[\"code\"], sample[\"name\"]\n",
    "    runtime_stats = measure_program_time(\n",
    "        ref_arch_name=ref_arch_name,\n",
    "        ref_arch_src=ref_arch_src,\n",
    "        use_torch_compile=use_torch_compile,\n",
    "        torch_compile_backend=torch_compile_backend,\n",
    "        torch_compile_options=torch_compile_options,\n",
    "        device=device,\n",
    "        verbose=False, # do not print \n",
    "        num_trials=num_trials,\n",
    "    )\n",
    "    json_results[f\"level_{level}\"][ref_arch_name] = runtime_stats\n",
    "\n",
    "save_path = os.path.join(TIMING_DIR, file_name)\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(json_results, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_kb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
