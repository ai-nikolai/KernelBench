{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f2f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbed7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from the \"eval.py\" file.\n",
    "def set_seed(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    # NOTE: this only sets on current cuda device\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    \n",
    "def load_original_model_and_inputs(\n",
    "    model_original_src: str, context: dict\n",
    ") -> tuple[nn.Module, callable, callable]:\n",
    "    \"\"\"\n",
    "    Load class from original NN.module pytorch code\n",
    "    this is pytorch reference and we feed that to model to see if there will be any improvement\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        compile(model_original_src, \"<string>\", \"exec\")\n",
    "    except SyntaxError as e:\n",
    "        print(f\"Syntax Error in original code {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        exec(model_original_src, context)  # expose to current namespace\n",
    "    except Exception as e:\n",
    "        print(f\"Error in executing original code {e}\")\n",
    "        return None\n",
    "\n",
    "    # these should be defined in the original model code and present in the context\n",
    "    get_init_inputs_fn = context.get(\"get_init_inputs\")\n",
    "    get_inputs_fn = context.get(\"get_inputs\")\n",
    "    Model = context.get(\"Model\")\n",
    "    return (Model, get_init_inputs_fn, get_inputs_fn)\n",
    "\n",
    "def time_execution_with_cuda_event(\n",
    "    kernel_fn: callable,\n",
    "    *args,\n",
    "    num_warmup: int = 3,\n",
    "    num_trials: int = 10,\n",
    "    verbose: bool = True,\n",
    "    device: torch.device = None,\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Time a CUDA kernel function over multiple trials using torch.cuda.Event\n",
    "\n",
    "    Args:\n",
    "        kernel_fn: Function to time\n",
    "        *args: Arguments to pass to kernel_fn\n",
    "        num_trials: Number of timing trials to run\n",
    "        verbose: Whether to print per-trial timing info\n",
    "        device: CUDA device to use, if None, use current device\n",
    "\n",
    "    Returns:\n",
    "        List of elapsed times in milliseconds\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        if verbose:\n",
    "            print(f\"Using current device: {torch.cuda.current_device()}\")\n",
    "        device = torch.cuda.current_device()\n",
    "\n",
    "    # Warm ups\n",
    "    for _ in range(num_warmup):\n",
    "        kernel_fn(*args)\n",
    "        torch.cuda.synchronize(device=device)\n",
    "\n",
    "    print(\n",
    "        f\"[Profiling] Using device: {device} {torch.cuda.get_device_name(device)}, warm up {num_warmup}, trials {num_trials}\"\n",
    "    )\n",
    "    elapsed_times = []\n",
    "\n",
    "    # Actual trials\n",
    "    for trial in range(num_trials):\n",
    "        # create event marker default is not interprocess\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start_event.record()\n",
    "        kernel_fn(*args)\n",
    "        end_event.record()\n",
    "\n",
    "        # Synchronize to ensure the events have completed\n",
    "        torch.cuda.synchronize(device=device)\n",
    "\n",
    "        # Calculate the elapsed time in milliseconds\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "        if verbose:\n",
    "            print(f\"Trial {trial + 1}: {elapsed_time_ms:.3g} ms\")\n",
    "        elapsed_times.append(elapsed_time_ms)\n",
    "\n",
    "    return elapsed_times\n",
    "\n",
    "def get_timing_stats(elapsed_times: list[float], device: torch.device = None) -> dict:\n",
    "    \"\"\"Get timing statistics from a list of elapsed times.\n",
    "\n",
    "    Args:\n",
    "        elapsed_times: List of elapsed times in milliseconds\n",
    "        device: CUDA device, record device info\n",
    "    Returns:\n",
    "        Dict containing mean, std, min, max and num_trials\n",
    "        all timing are in ms\n",
    "    \"\"\"\n",
    "\n",
    "    stats = {\n",
    "        \"mean\": float(f\"{np.mean(elapsed_times):.3g}\"),\n",
    "        \"std\": float(f\"{np.std(elapsed_times):.3g}\"),\n",
    "        \"min\": float(f\"{np.min(elapsed_times):.3g}\"),\n",
    "        \"max\": float(f\"{np.max(elapsed_times):.3g}\"),\n",
    "        \"num_trials\": len(elapsed_times),\n",
    "    }\n",
    "\n",
    "    if device:\n",
    "        stats[\"hardware\"] = torch.cuda.get_device_name(device=device)\n",
    "        stats[\"device\"] = str(device)  # for debugging\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c343e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the original file \"generate_baseline_time.py\"\n",
    "def measure_program_time(\n",
    "        ref_arch_name: str,\n",
    "        ref_arch_src: str, \n",
    "        num_trials: int = 100,\n",
    "        use_torch_compile: bool = False,\n",
    "        torch_compile_backend: str=\"inductor\", \n",
    "        torch_compile_options: str=\"default\",\n",
    "        device: torch.device=\"cuda:0\",\n",
    "        verbose: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Measure the time of a KernelBench reference architecture\n",
    "    \"\"\"\n",
    "    context = {}\n",
    "    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(\n",
    "        ref_arch_src, context\n",
    "    )\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.synchronize(device=device)\n",
    "            set_seed(42)\n",
    "            inputs = get_inputs()\n",
    "            set_seed(42)\n",
    "            init_inputs = get_init_inputs()\n",
    "            inputs = [\n",
    "                x.cuda(device=device) if isinstance(x, torch.Tensor) else x\n",
    "                for x in inputs\n",
    "            ]\n",
    "            init_inputs = [\n",
    "                x.cuda(device=device) if isinstance(x, torch.Tensor) else x\n",
    "                for x in init_inputs\n",
    "            ]\n",
    "\n",
    "            # Initialize PyTorch model, use this for eager mode execution\n",
    "            model = Model(*init_inputs)\n",
    "            \n",
    "            if use_torch_compile:\n",
    "                print(f\"Using torch.compile to compile model {ref_arch_name} with {torch_compile_backend} backend and {torch_compile_options} mode\")\n",
    "                model = torch.compile(model, backend=torch_compile_backend, mode=torch_compile_options)\n",
    "            else:\n",
    "                print(f\"Using PyTorch Eager Execution on {ref_arch_name}\")\n",
    "            \n",
    "            model = model.cuda(device=device)\n",
    "            torch.cuda.synchronize(device=device)\n",
    "            elapsed_times = time_execution_with_cuda_event(\n",
    "                model, *inputs, num_trials=num_trials, verbose=verbose, device=device\n",
    "            )\n",
    "            runtime_stats = get_timing_stats(elapsed_times, device=device)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{ref_arch_name} {runtime_stats}\")\n",
    "            \n",
    "            return runtime_stats\n",
    "    except Exception as e:\n",
    "        print(f\"[Eval] Error in Measuring Performance: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98287bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional Functions (eval.py and others?)\n",
    "\n",
    "def gpu_cache_clean(device: torch.device = torch.device(\"cuda:0\")):\n",
    "    \"\"\"Cleans up the GPU cache.\"\"\"\n",
    "    # Clear CUDA cache and reset GPU state\n",
    "    with torch.cuda.device(device):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # does this help?\n",
    "        torch.cuda.reset_peak_memory_stats(device=device)\n",
    "\n",
    "        torch.cuda.synchronize(\n",
    "            device=device\n",
    "        )  # Wait for all CUDA operations to complete\n",
    "\n",
    "def graceful_eval_cleanup(curr_context: dict, device: torch.device):\n",
    "    \"\"\"\n",
    "    Clean up env, gpu cache, and compiled CUDA extensions after evaluation\n",
    "    \"\"\"  # delete ran-specific function definitions before next eval run\n",
    "    del curr_context\n",
    "    gpu_cache_clean(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13c0c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 100_HingeLoss\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:08<05:40,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 10_3D_tensor_matrix_multiplication\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:10<04:19,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 11_4D_tensor_matrix_multiplication\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:10<02:40,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 12_Matmul_with_diagonal_matrices_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:10<01:59,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 13_Matmul_for_symmetric_matrices\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:11<01:27,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 14_Matmul_for_upper_triangular_matrices\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:11<01:10,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 15_Matmul_for_lower_triangular_matrices\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:11<00:59,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 16_Matmul_with_transposed_A\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:12<00:51,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 17_Matmul_with_transposed_B\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:12<00:46,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 18_Matmul_with_transposed_both\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:24<05:56,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 19_ReLU\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:24<04:12,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 1_Square_matrix_multiplication_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:35<07:38,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 20_LeakyReLU\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:45<09:42,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 21_Sigmoid\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:55<10:49,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 22_Tanh\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 23_Softmax\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [01:05<11:50,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 24_LogSoftmax\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [01:16<12:24,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 25_Swish\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [01:38<13:34, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 26_GELU_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [01:49<13:58, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 27_SELU_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [02:00<13:49, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 28_HardSigmoid\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [02:10<13:45, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 29_Softplus\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [02:11<09:39,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 2_Standard_matrix_multiplication_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 30_Softsign\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [02:33<11:29,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 31_ELU\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [02:43<11:51,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 32_HardTanh\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [02:50<10:49,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 33_BatchNorm\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 34_InstanceNorm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [03:03<12:02, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 35_GroupNorm_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [03:15<12:38, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 36_RMSNorm_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [03:27<12:55, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 37_FrobeniusNorm_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [03:40<13:09, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 38_L1Norm_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [03:53<13:44, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 39_L2Norm_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [04:09<10:17,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 3_Batched_matrix_multiplication\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [04:09<07:16,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 40_LayerNorm\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [04:15<06:52,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 41_Max_Pooling_1D\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [04:18<05:49,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 42_Max_Pooling_2D\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [04:25<06:05,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 43_Max_Pooling_3D\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [04:29<05:13,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 44_Average_Pooling_1D\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 45_Average_Pooling_2D\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [05:09<12:00, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 46_Average_Pooling_3D\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [05:22<12:05, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 47_Sum_reduction_over_a_dimension\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [05:36<12:06, 12.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 48_Mean_reduction_over_a_dimension\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [05:48<11:51, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 49_Max_reduction_over_a_dimension\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [06:03<12:11, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 4_Matrix_vector_multiplication_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [06:03<08:28,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 50_conv_standard_2D__square_input__square_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [06:18<09:52, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 51_Argmax_over_a_dimension\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [06:32<10:21, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 52_Argmin_over_a_dimension\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [06:46<10:31, 12.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 53_Min_reduction_over_a_dimension\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 54_conv_standard_3D__square_input__square_kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [06:46<07:15,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [06:48<05:23,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 55_conv_standard_2D__asymmetric_input__square_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [06:48<03:50,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 56_conv_standard_2D__asymmetric_input__asymmetric_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [06:51<03:25,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 57_conv_transposed_2D__square_input__square_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [06:52<02:24,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 58_conv_transposed_3D__asymmetric_input__asymmetric_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 59_conv_standard_3D__asymmetric_input__square_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [07:00<02:57,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 5_Matrix_scalar_multiplication\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 60_conv_standard_3D__square_input__asymmetric_kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [07:00<02:03,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [07:01<01:33,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 61_conv_transposed_3D__square_input__square_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [07:02<01:09,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 62_conv_standard_2D__square_input__asymmetric_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 63_conv_standard_2D__square_input__square_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [07:07<01:28,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 64_conv_transposed_1D\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [07:08<01:17,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 65_conv_transposed_2D__square_input__asymmetric_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 66_conv_standard_3D__asymmetric_input__asymmetric_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [07:10<00:56,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 67_conv_standard_1D\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 68_conv_transposed_3D__square_input__asymmetric_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [07:15<00:59,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 69_conv_transposed_2D__asymmetric_input__asymmetric_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [07:17<00:57,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 6_Matmul_with_large_K_dimension_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 70_conv_transposed_3D__asymmetric_input__square_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [07:20<00:52,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 71_conv_transposed_2D__asymmetric_input__square_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 72_conv_transposed_3D_asymmetric_input_asymmetric_kernel___strided_padded_grouped_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [07:20<00:29,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 73_conv_transposed_3D_asymmetric_input_square_kernel__strided_padded__grouped\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [07:22<00:28,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 74_conv_transposed_1D_dilated\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [07:22<00:22,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 75_conv_transposed_2D_asymmetric_input_asymmetric_kernel_strided__grouped____padded____dilated__\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [07:35<01:48,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 76_conv_standard_1D_dilated_strided__\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 77_conv_transposed_3D_square_input_square_kernel___padded____dilated____strided__\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [07:36<01:02,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 78_conv_transposed_2D_asymmetric_input_asymmetric_kernel___padded__\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [07:37<00:48,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 79_conv_transposed_1D_asymmetric_input_square_kernel___padded____strided____dilated__\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 7_Matmul_with_small_K_dimension_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [07:37<00:28,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 80_conv_standard_2D_square_input_asymmetric_kernel___dilated____padded__\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 81_conv_transposed_2D_asymmetric_input_square_kernel___dilated____padded____strided__\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [07:39<00:23,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 82_conv_depthwise_2D_square_input_square_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [07:40<00:21,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 83_conv_depthwise_2D_square_input_asymmetric_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [07:48<00:44,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 84_conv_depthwise_2D_asymmetric_input_square_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [07:49<00:35,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 85_conv_depthwise_2D_asymmetric_input_asymmetric_kernel\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [07:50<00:31,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 86_conv_depthwise_separable_2D\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [07:57<00:46,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 87_conv_pointwise_2D\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [07:58<00:32,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 88_MinGPTNewGelu\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [08:04<00:44,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 89_cumsum\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [08:05<00:30,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 8_Matmul_with_irregular_shapes_\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [08:12<00:40,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 90_cumprod\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 91_cumsum_reverse\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [08:18<00:43,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 92_cumsum_exclusive\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [08:26<00:45,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 93_masked_cumsum\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [08:42<01:00,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 94_MSELoss\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [08:59<00:39,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 95_CrossEntropyLoss\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [09:16<00:42, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 96_HuberLoss\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 97_ScaledDotProductAttention\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [09:21<00:13,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 98_KLDivLoss\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [09:27<00:06,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 99_TripletMarginLoss\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n",
      "Using PyTorch Eager Execution on 9_Tall_skinny_matrix_multiplication_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:27<00:00,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpu_cache_clean()\n",
    "TIMING_DIR = \"./timings\"\n",
    "file_name: str=\"baseline_time_v2.json\"\n",
    "\n",
    "\n",
    "# MAIN SCRIPT\n",
    "ds = load_dataset(\"ai-nikolai/KernelBench\")\n",
    "\n",
    "#SOME INIT PARAMS\n",
    "num_trials: int= 3\n",
    "\n",
    "use_torch_compile: bool = False\n",
    "torch_compile_backend: str=\"inductor\"\n",
    "torch_compile_options: str=\"default\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "json_results = {}\n",
    "\n",
    "#\n",
    "level=1\n",
    "level_1 = ds[f\"level_{level}\"]\n",
    "num_problems = len(level_1)\n",
    "\n",
    "json_results[f\"level_{level}\"]={}\n",
    "\n",
    "for sample in tqdm.tqdm(level_1):\n",
    "    ref_arch_src, ref_arch_name  = sample[\"code\"], sample[\"name\"]\n",
    "    runtime_stats = measure_program_time(\n",
    "        ref_arch_name=ref_arch_name,\n",
    "        ref_arch_src=ref_arch_src,\n",
    "        use_torch_compile=use_torch_compile,\n",
    "        torch_compile_backend=torch_compile_backend,\n",
    "        torch_compile_options=torch_compile_options,\n",
    "        device=device,\n",
    "        verbose=False, # do not print \n",
    "        num_trials=num_trials,\n",
    "    )\n",
    "    json_results[f\"level_{level}\"][ref_arch_name] = runtime_stats\n",
    "\n",
    "save_path = os.path.join(TIMING_DIR, file_name)\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(json_results, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_kb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
