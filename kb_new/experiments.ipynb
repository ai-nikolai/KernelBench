{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f2f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfbed7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from the \"eval.py\" file.\n",
    "def set_seed(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    # NOTE: this only sets on current cuda device\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    \n",
    "def load_original_model_and_inputs(\n",
    "    model_original_src: str, context: dict\n",
    ") -> tuple[nn.Module, callable, callable]:\n",
    "    \"\"\"\n",
    "    Load class from original NN.module pytorch code\n",
    "    this is pytorch reference and we feed that to model to see if there will be any improvement\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        compile(model_original_src, \"<string>\", \"exec\")\n",
    "    except SyntaxError as e:\n",
    "        print(f\"Syntax Error in original code {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        exec(model_original_src, context)  # expose to current namespace\n",
    "    except Exception as e:\n",
    "        print(f\"Error in executing original code {e}\")\n",
    "        return None\n",
    "\n",
    "    # these should be defined in the original model code and present in the context\n",
    "    get_init_inputs_fn = context.get(\"get_init_inputs\")\n",
    "    get_inputs_fn = context.get(\"get_inputs\")\n",
    "    Model = context.get(\"Model\")\n",
    "    return (Model, get_init_inputs_fn, get_inputs_fn)\n",
    "\n",
    "def time_execution_with_cuda_event(\n",
    "    kernel_fn: callable,\n",
    "    *args,\n",
    "    num_warmup: int = 3,\n",
    "    num_trials: int = 10,\n",
    "    verbose: bool = True,\n",
    "    device: torch.device = None,\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Time a CUDA kernel function over multiple trials using torch.cuda.Event\n",
    "\n",
    "    Args:\n",
    "        kernel_fn: Function to time\n",
    "        *args: Arguments to pass to kernel_fn\n",
    "        num_trials: Number of timing trials to run\n",
    "        verbose: Whether to print per-trial timing info\n",
    "        device: CUDA device to use, if None, use current device\n",
    "\n",
    "    Returns:\n",
    "        List of elapsed times in milliseconds\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        if verbose:\n",
    "            print(f\"Using current device: {torch.cuda.current_device()}\")\n",
    "        device = torch.cuda.current_device()\n",
    "\n",
    "    # Warm ups\n",
    "    for _ in range(num_warmup):\n",
    "        kernel_fn(*args)\n",
    "        torch.cuda.synchronize(device=device)\n",
    "\n",
    "    print(\n",
    "        f\"[Profiling] Using device: {device} {torch.cuda.get_device_name(device)}, warm up {num_warmup}, trials {num_trials}\"\n",
    "    )\n",
    "    elapsed_times = []\n",
    "\n",
    "    # Actual trials\n",
    "    for trial in range(num_trials):\n",
    "        # create event marker default is not interprocess\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        start_event.record()\n",
    "        kernel_fn(*args)\n",
    "        end_event.record()\n",
    "\n",
    "        # Synchronize to ensure the events have completed\n",
    "        torch.cuda.synchronize(device=device)\n",
    "\n",
    "        # Calculate the elapsed time in milliseconds\n",
    "        elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "        if verbose:\n",
    "            print(f\"Trial {trial + 1}: {elapsed_time_ms:.3g} ms\")\n",
    "        elapsed_times.append(elapsed_time_ms)\n",
    "\n",
    "    return elapsed_times\n",
    "\n",
    "def get_timing_stats(elapsed_times: list[float], device: torch.device = None) -> dict:\n",
    "    \"\"\"Get timing statistics from a list of elapsed times.\n",
    "\n",
    "    Args:\n",
    "        elapsed_times: List of elapsed times in milliseconds\n",
    "        device: CUDA device, record device info\n",
    "    Returns:\n",
    "        Dict containing mean, std, min, max and num_trials\n",
    "        all timing are in ms\n",
    "    \"\"\"\n",
    "\n",
    "    stats = {\n",
    "        \"mean\": float(f\"{np.mean(elapsed_times):.3g}\"),\n",
    "        \"std\": float(f\"{np.std(elapsed_times):.3g}\"),\n",
    "        \"min\": float(f\"{np.min(elapsed_times):.3g}\"),\n",
    "        \"max\": float(f\"{np.max(elapsed_times):.3g}\"),\n",
    "        \"num_trials\": len(elapsed_times),\n",
    "    }\n",
    "\n",
    "    if device:\n",
    "        stats[\"hardware\"] = torch.cuda.get_device_name(device=device)\n",
    "        stats[\"device\"] = str(device)  # for debugging\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c343e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the original file \"generate_baseline_time.py\"\n",
    "def measure_program_time(\n",
    "        ref_arch_name: str,\n",
    "        ref_arch_src: str, \n",
    "        num_trials: int = 100,\n",
    "        use_torch_compile: bool = False,\n",
    "        torch_compile_backend: str=\"inductor\", \n",
    "        torch_compile_options: str=\"default\",\n",
    "        device: torch.device=\"cuda:0\",\n",
    "        verbose: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Measure the time of a KernelBench reference architecture\n",
    "    \"\"\"\n",
    "    context = {}\n",
    "    Model, get_init_inputs, get_inputs = load_original_model_and_inputs(\n",
    "        ref_arch_src, context\n",
    "    )\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.synchronize(device=device)\n",
    "            set_seed(42)\n",
    "            inputs = get_inputs()\n",
    "            set_seed(42)\n",
    "            init_inputs = get_init_inputs()\n",
    "            inputs = [\n",
    "                x.cuda(device=device) if isinstance(x, torch.Tensor) else x\n",
    "                for x in inputs\n",
    "            ]\n",
    "            init_inputs = [\n",
    "                x.cuda(device=device) if isinstance(x, torch.Tensor) else x\n",
    "                for x in init_inputs\n",
    "            ]\n",
    "\n",
    "            # Initialize PyTorch model, use this for eager mode execution\n",
    "            model = Model(*init_inputs)\n",
    "            \n",
    "            if use_torch_compile:\n",
    "                print(f\"Using torch.compile to compile model {ref_arch_name} with {torch_compile_backend} backend and {torch_compile_options} mode\")\n",
    "                model = torch.compile(model, backend=torch_compile_backend, mode=torch_compile_options)\n",
    "            else:\n",
    "                print(f\"Using PyTorch Eager Execution on {ref_arch_name}\")\n",
    "            \n",
    "            model = model.cuda(device=device)\n",
    "            torch.cuda.synchronize(device=device)\n",
    "            elapsed_times = time_execution_with_cuda_event(\n",
    "                model, *inputs, num_trials=num_trials, verbose=verbose, device=device\n",
    "            )\n",
    "            runtime_stats = get_timing_stats(elapsed_times, device=device)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{ref_arch_name} {runtime_stats}\")\n",
    "            \n",
    "            return runtime_stats\n",
    "    except Exception as e:\n",
    "        print(f\"[Eval] Error in Measuring Performance: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Eager Execution on 100_HingeLoss\n",
      "[Profiling] Using device: cuda:0 NVIDIA A40, warm up 3, trials 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'level1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     21\u001b[39m     ref_arch_src, ref_arch_name  = sample[\u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m], sample[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     22\u001b[39m     runtime_stats = measure_program_time(\n\u001b[32m     23\u001b[39m         ref_arch_name=ref_arch_name,\n\u001b[32m     24\u001b[39m         ref_arch_src=ref_arch_src,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m         num_trials=num_trials,\n\u001b[32m     31\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[43mjson_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlevel\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlevel\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[ref_arch_name] = runtime_stats\n\u001b[32m     34\u001b[39m save_path = os.path.join(TIMING_DIR, file_name)\n\u001b[32m     35\u001b[39m os.makedirs(os.path.dirname(save_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyError\u001b[39m: 'level1'"
     ]
    }
   ],
   "source": [
    "TIMING_DIR = \"./timings\"\n",
    "ds = load_dataset(\"ai-nikolai/KernelBench\")\n",
    "\n",
    "#SOME INIT PARAMS\n",
    "file_name: str=\"baseline_time.json\"\n",
    "num_trials: int= 100\n",
    "\n",
    "use_torch_compile: bool = False\n",
    "torch_compile_backend: str=\"inductor\"\n",
    "torch_compile_options: str=\"default\"\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "json_results = {}\n",
    "\n",
    "#\n",
    "level=1\n",
    "level_1 = ds[f\"level_{level}\"]\n",
    "num_problems = len(level_1)\n",
    "\n",
    "json_results[f\"level_{level}\"]={}\n",
    "\n",
    "for sample in tqdm.tqdm(level_1):\n",
    "    ref_arch_src, ref_arch_name  = sample[\"code\"], sample[\"name\"]\n",
    "    runtime_stats = measure_program_time(\n",
    "        ref_arch_name=ref_arch_name,\n",
    "        ref_arch_src=ref_arch_src,\n",
    "        use_torch_compile=use_torch_compile,\n",
    "        torch_compile_backend=torch_compile_backend,\n",
    "        torch_compile_options=torch_compile_options,\n",
    "        device=device,\n",
    "        verbose=False, # do not print \n",
    "        num_trials=num_trials,\n",
    "    )\n",
    "    json_results[f\"level_{level}\"][ref_arch_name] = runtime_stats\n",
    "\n",
    "save_path = os.path.join(TIMING_DIR, file_name)\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(json_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 24.96ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.14s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 483.33ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.15 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 333.04ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.16s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 518.90ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.20 shards/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ai-nikolai/KernelBench/commit/d55e90db696ff59275c3754da6d68471b106ab13', commit_message='Upload dataset', commit_description='', oid='d55e90db696ff59275c3754da6d68471b106ab13', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ai-nikolai/KernelBench', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ai-nikolai/KernelBench'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds[\"level_1\"]\n",
    "# ds.push_to_hub(\"KernelBench\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_kb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
